\section{Related Work}
\label{chap:relatedwork}

In this section, we review related work on erasure coding, erasure-coded
storage, log-structured file systems and parity logging.

\subsection{Erasure-coded Storage}

Traditionally, clustered storage systems adopt replication to provide data
redundancy. GFS \cite{ghemawat03} first proposes to store three replicas for
data, which is adopted by subsequent designs including Ceph \cite{weil06} and
Hadoop \cite{shvachko10}.

Quantitative analysis shows that erasure coding consumes less bandwidth and
storage than replication with similar system durability
\cite{weatherspoon02,rodrigues05}. Under the same storage overhead, it is shown
that the \textit{mean time to failure} (MTTF) for storage systems utilizing
erasure coding can be orders of magnitude higher than those using replication
\cite{weatherspoon02}.

Therefore, several studies adopt erasure coding in clustered storage systems.
OceanStore \cite{kubiatowicz00,rhea03} combines replication and erasure coding
for wide-area network storage.  It uses a Byzantine-fault tolerant model to
provide strong consistency among replicas.

TotalRecall \cite{bhagwan04} applies replication or erasure coding to different
files dynamically according to the availability level predicted by the system.
In addition to the level of redundancy, TotalRecall switches between (1)
\textit{eager repair}, in which the level of redundancy is maintained by
immediate repair action after a failure, and (2) \textit{lazy repair}, in which the
additional redundancy is temporarily used to delay repair action.

Ursa Minor \cite{abd05} focuses on cluster storage and encodes files of
heterogeneous types based on the failure models and access patterns. It adapts
to changing workload pattern by supporting online re-encoding of data. In
addition, it supports versioning for its objects.

Panasas \cite{welch08} performs client-side encoding on a per-file basis. It is
shown by extensive experiments that by moving the computation to clients, the
parity computation power of the system scales up as the number of clients
increases. In addition, this design can be extended with \textit{end-to-end
    integrity} by computing and verifying checksum at both ends of the write
paths.

Unlike Panasas, some designs such as TickerTAIP \cite{cao94}, PARAID
\cite{weddle07} and Pergamum \cite{storer08} offload the parity computation to
the storage array. TickerTAIP utilizes a group of \textit{intelligent} worker
nodes, which are nodes with coding ability, connected in a small-area network.
This is similar to the concept of \textit{object storage devices} (OSDs) in
modern context. PARAID is designed for a single-host RAID. It achieves
energy-savings using a skewed stripping pattern and spinning down unused disks
during light loads. Pergamum also focuses on energy optimization. It adds
non-volatile memory (NVRAM) to each node in order to defer metadata updates and
small writes to powered off disks.

R-Admad \cite{liu09} enhances the reliability of a deduplication
system by adopting erasure coding to store deduplicated data. It speeds up
recovery by distributing the repair workload among the storage nodes.

Zhang \etal \cite{zhang10} modify HDFS with inline erasure coding and
evaluate different workloads atop erasure-coded data.

CAROM \cite{ma13} uses replication for caching in its erasure coded distributed
storage system so as to benefit high throughput from replicated cache and low
storage overhead from erasure-coded data. It redirects all writes to a primary
replica to ensure cache consistency. 

Azure \cite{huang12} and Facebook \cite{sathiamoorthy13} propose efficient
erasure coding schemes to speed up degraded reads. 

% But they perform coding off the write path by first performing replication
% and later transforming replicas into their erasure-coded form. 
We complement the above studies by improving update efficiency and recovery
performance in erasure-coded clustered storage. 
%data modification in its encoded form. In addition, CodFS minimizes network
%traffic between clients and the storage cluster by offloading encoding
%overhead to the server-side.


\subsection{Log-structured File System}

%Zebra

Log-structured File System (LFS) \cite{rosenblum92} first proposes to append
updates sequentially to disk to improve write performance. LFS buffers all 
updates and metadata in an in-memory segment, and flushes the segment to disk
using a large sequential write.

Zebra \cite{hartman95} extends LFS for RAID-like distributed storage systems by
striping logs across servers. Zebra maintains an append-only log for each client
and mitigates parity update overhead by computing parity for the log instead of
the files. 

AutoRAID \cite{wilkes95} uses a workload-aware approach to store active data
using replication (RAID-1) and inactive data using erasure coding (RAID-5). In
the RAID-5 layer, it batches updates in a log to mitigate read-modify write
overhead in RAID.

Several studies focus on designing more efficient garbage collection algorithms
for LFS. Self-tuning LFS \cite{matthews97} exploits workload characteristics to
improve I/O performance. It speeds up garbage collection in LFS by dynamically
choosing between two cleaning algorithms, (1) traditional cleaning, which
combine live data from several partially empty segments to form a full segment
and (2) hole-plugging, which copy live data to holes found in other segments.
BSD-LFS \cite{seltzer93,seltzer95} simplifies LFS by moving the cleaner from
kernel space to user space. It also allows multiple cleaners with different
cleaning policies to execute in parallel.

Clustered storage systems, GFS \cite{ghemawat03} and Azure \cite{calder11}, also
adopt the LFS design for the write-once read-many workload.  The more recent
work Gecko \cite{shin13} uses a chained-log design to reduce disk I/O contention
of LFS in RAID storage.  

The LFS design is also adopted in other types of emerging storage media, such as
SSDs \cite{agrawal08} and DRAM storage \cite{ongaro11}.  RAMCloud
\cite{ongaro11} keeps all data
permanently on DRAM and exploits scalability in recovery by distributing data
randomly over a large number of nodes. 

CodFS handles updates differently from LFS, in which it performs in-place
updates to data and log-based updates to parity chunks.  It also allocates
reserve space for parity logging to further mitigate disk seeks (see Chapter
\ref{chap:codfs}).  On the other
hand, TWEEN exploits the LFS design to improve write performance and endurance
of SSD RAID (see Chapter \ref{chap:tween}).


%Both LFS and CodFS aim to reducing the amount of time that a disk spends on
%seeking.  Unlike Zebra, CodFS performs in-place updates to data and only
%applies logging for parity blocks. With the help of a reserve space, CodFS
%achieves significantly shorter recovery time by minimizing fragmentation in
%both data and parity blocks. 


%Both RAMCloud \cite{ongaro11} and Gecko \cite{shin13} apply a log-structured
%approach for its data. RAMCloud keeps all data permanently on DRAM which does
%not suffer from the fragmentation problem we present.  However, as admitted
%in the paper, RAMCloud is not yet a practical large-scale storage solution
%for large media files due to the high cost of DRAM. Gecko proposed a
%chained-log design to minimize I/O contention occurring in typical RAID
%systems and LFS, but their design is limited to disks in a single host only.
%Also, write throughput of Gecko is limited by the bandwidth of the tail disk.
%CodFS emphasizes on scalability by utilizing the bandwidth of all storage
%nodes. 

\subsection{Parity Logging}

%Traditional RAID mainly use two approaches to update parity chunks during
%writes \cite{thomasian05}: Read-modify write and reconstruct write are used 
%Both approaches are I/O expensive for small updates.  
Parity logging \cite{chen94,stodolsky93} has been proposed to mitigate the disk
seek overhead in parity updates caused by small writes. The main idea is to
delay the read of the old parity and the write of the new parity by storing the
difference between the old and new parity (or parity updates) temporarily in a
log. It appends parity updates for each parity region in a log and flushes
updates to the parity region when the log is full.  The parity and log regions
can be distributed across all disks \cite{stodolsky93}.  On the other hand,
CodFS reserves log space next to each parity chunk so as to reduce disk seeks
due to frequent small writes.  It extends the prior parity logging approaches by
allowing future shrinking of the reserved space based on the workload. 
%which predefines a fixed-size log region for each parity region, CodFS
%reserves the log space on the write path and allows future shrinking of the
%reserved space depending on the workload.
%We also CodFS also drives the experiments by real-world trace.

%Our full-overwrite scheme is similar to the approaches in the studies
%\cite{aguilera05,frolund04,zhang12}.  CodFS complements them by further
%comparing the fully-overwrite scheme with other update schemes, and proposes
%the parity-logging with reserved space scheme that reduces disk fragmentation
%and achieves higher I/O throughput. 

%similar update schemes to the fully-overwrite mode in CodFS for their
%erasure-coded storage systems. Aguilera's and Frolund's main focus are to
%maintain entire system consistency under their update schemes while
%guaranteeing good performance. Zhang compared their update scheme with
%traditional reconstruction-write and read-modify-write to illustrate the
%improvements. 

