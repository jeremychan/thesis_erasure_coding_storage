\chapter{Introduction}
\label{sec:introduction}

In this chapter, we give an overview of our project and highlight our
contributions.

\section{Overview}

\subsection{CodFS}

% why erasure coding
Clustered storage systems are known to be susceptible to component failures
\cite{ghemawat03}.  High data availability can be achieved by encoding data
with redundancy using either replication or erasure coding.  Erasure coding
encodes original data chunks to generate new parity chunks, such that a subset
of data and parity chunks can sufficiently recover all original data chunks.
It is known that erasure coding introduces less overhead in storage and write
bandwidth than replication under the same fault tolerance
\cite{weatherspoon02,rodrigues05}.  For example, traditional 3-way replication
used in GFS \cite{ghemawat03} and Azure \cite{calder11} introduces 200\% of
redundancy overhead, while erasure coding can reduce the overhead to 33\%
and achieve higher availability \cite{huang12}.  Today's enterprise
clustered storage systems
\cite{welch08,ford10,huang12,sathiamoorthy13,resch11} adopt erasure coding in
production to reduce hardware footprints and maintenance costs.

%While erasure coding is promising and sees practical deployment, existing
%systems target specific domains of workloads and implementations, and several
%design trade-off issues remain open.  Here, we identify three: 
%\begin{itemize}
%\item
%{\bf Out-of-order versus inline erasure coding.}  Existing distributed file
%systems perform {\em out-of-order} erasure coding
%\cite{ford10,huang12,sathiamoorthy13}.  That is, data is first replicated, and
%the replicas are later erasure-coded in the background.  However, the
%out-of-order approach incurs extra I/Os since it first writes replicas and
%then transforming them into erasure-coded data.  An alternative is to perform
%inline erasure coding on the write path, but it degrades read performance
%since read requests cannot be easily load balanced across multiple replicas as
%in replication \cite{fan09}. 
%\item
% why write-many
%{\bf Write-once versus write-many workloads.} Many distributed file systems
%such as GFS \cite{ghemawat03}, HDFS \cite{shvachko10} and Azure \cite{huang12}
%assume write-once-read-many (WORM) semantics on files, which become immutable
%once closed.  This model is valid for cloud computing workloads, such as
%MapReduce applications \cite{dean04}.  On the other hand, for many real-world
%workloads in enterprise servers and network file systems
%\cite{adams12,narayanan08}, data updates are common, and the
%write-many-read-many (WMRM) semantics need to supported in the file system.
%Erasure coding introduces high overhead in the WMRM model, since the 
%{\em parity} chunks (i.e., chunks encoded from data) need to be
%re-encoded for the data changes. 
%Given the needs for a write-many inline erasure coding file system, we need
%designs that provide update efficiency. Efficient update will decrease the
%I/O latency and improve the overall application response time. 
%\item
%{\bf Out-of-order versus in-place updates.} 
% why fast recovery

For many real-world workloads in enterprise servers and network file systems
\cite{adams12,narayanan08}, data updates are dominant.  There are two ways of
performing updates: (1) {\em in-place updates}, where the stored data is read,
modified, and written with the new data, and (2) {\em log-based updates},
where updates are inserted to the end of an append-only log
\cite{rosenblum92}.  If updates are frequent, in-place updates introduce
significant I/O overhead in erasure-coded storage since parity chunks also
need to be updated to be consistent with the data changes.  Existing
clustered storage systems, such as GFS \cite{ghemawat03} and Azure 
\cite{calder11} adopt log-based updates to reduce \mbox{I/Os} by sequentially
appending updates.  On the other hand, log-based updates introduce additional
disk seeks to the update log during sequential reads.  This in particular
hurts recovery performance, since recovery makes large sequential reads to the
data and parity chunks in the surviving nodes in order to reconstruct the lost
data. 
%The issue becomes more severe for erasure-coded storage, because the updates
%must be first read and reassembled from different coded chunks before the
%lost data can be reconstructed. 

This raises an issue of choosing the appropriate update scheme for an
erasure-coded clustered storage system to achieve efficient updates and
recovery simultaneously.  Our primary goal is to mitigate the network transfer
and disk I/O overheads, both of which are potential bottlenecks in clustered
storage systems. 
We propose and build an erasure-coded clustered storage system {\em CodFS}, 
which supports efficient updates and recovery. 
%However, such approach generates disk seeks for both data and parity blocks
%during update, which does not meet the update efficiency that we require in
%our inline erasure coding system.
%On the other hand, recovery time plays in significant role in deciding the
%availability and reliability of a distributed file system. Longer recovery
%time prolongs the "window of vulnerability" of an erasure coding system,
%during which another node failure will render irrecoverable data loss
%\cite{xin03}. During recovery, large sequential reads are issued to both data
%and parity blocks in the surviving nodes in order to reconstruct the lost
%blocks. 
%\end{itemize}

% contributions

%First, we conduct trace analysis on real-world traces from production storage
%servers.  We characterize the update behaviors of the traces that drive our
%work. 

CodFS is a joint work with Qian Ding, Helen H. W. Chan and Patrick P. C. Lee.
Yan-kit Li and Chuan Qin also helped in the implementation of the early
prototype.

\subsection{TWEEN}

%Flash-based solid-state drives (SSDs) offer better I/O performance, shock
%resistance, and energy efficiency than traditional hard disk
%drives (HDDs), and have gained significant acceptance in desktop and server
%environments.  
%Modern SSDs embed error correction codes (ECCs) in the spare area of each
%flash page to protect against bit errors \cite{mielke08,grupp09,grupp12}.  
%However, the bit error rate of an SSD increases with hardware density
%\cite{lee11,grupp12} and the number of erase cycles that have been issued
%\cite{mielke08,grupp09}.    

To enhance the reliability of SSD systems beyond ECCs at the flash
level \cite{mielke08,grupp09,grupp12}, one option is to adopt 
parity-based RAID (\textit{Redundant Array of Inexpensive Disks})
\cite{patterson88} (e.g., RAID-4, RAID-5, RAID-6) at the device level. 

Nevertheless, when deploying parity-based SSD RAID (i.e., applying
parity-based RAID to multiple SSDs), we must pay special attention to small
random writes.  It is known that small random writes incur high garbage
collection overhead in SSDs and degrade the performance and endurance of SSDs
\cite{kim08,chen09,min12}.  Also, to maintain consistency between data and
parity, each data write triggers parity updates, which not only incur extra
I/Os that degrade write performance, but also further aggravate garbage
collection overhead in SSDs.  The primary problem of small random writes is
that they do not align the granularities of SSD garbage collection and RAID
encoding operations.  Such small random writes, which we term 
{\em partial writes}, incur high garbage collection overhead at the SSD level
and high parity update overhead at the RAID level.  Thus, for performance and
endurance enhancements of SSD RAID, eliminating partial writes is a critical
requirement. 

To eliminate partial writes, we propose TWEEN, which batches updates in a
log-structured file system (LFS) \cite{rosenblum92} until the aggregated write
size aligns the granularities at both SSD and RAID levels.  Our observation is
that byte-addressable non-volatile RAM (NVRAM) is an emerging storage substrate
\cite{jung10,venkataraman11,zhou09,qiu13,lee14}, and can serve as non-volatile
(persistent) cache for batching partial writes. This motivates us to realize a
practical and deployable design that combines LFS and NVRAM to eliminate partial
writes and hence enhance the performance and endurance of SSD RAID. 

TWEEN is a joint work with Qian Ding, Helen H. W. Chan and Patrick P. C. Lee.

\section{Contributions}

This thesis mainly focuses on the CodFS project. We highlight the following
contributions.

First, we provide a taxonomy of existing update schemes for
erasure-coded clustered storage systems.  To this end, we propose a novel
update scheme called {\em parity logging with reserved space}, which
uses a hybrid of in-place data updates and log-based parity updates.  It
mitigates the disk seeks of reading parity chunks by putting deltas of parity
chunks in a reserved space that is allocated next to their parity chunks.  We
further propose a workload-aware reserved space management scheme that
effectively predicts the size of reserved space and reclaims the unused
reserved space. 

Second, we build an erasure-coded clustered storage system CodFS, which
targets the common update-dominant workloads and supports efficient updates
and recovery.   CodFS offloads client-side encoding computations to the
storage cluster.  
%Specifically, a CodFS client distributes data across multiple storage nodes,
%each of which encodes the received data, and then serializes and
%re-distributes the encoded data to other storage nodes.  
Its implementation is extensible for different erasure coding and update
schemes, and is deployable on commodity hardware. 

Finally, we conduct testbed experiments using synthetic and real-world traces.
We show that our CodFS prototype achieves network-bound read/write
performance.  Under real-world workloads, our proposed parity logging with
reserved space gives a ${63.1}\%$ speedup of update throughput over pure
in-place updates and up to ${10\times}$ speedup of recovery throughput over
pure log-based updates.  Also, our workload-aware reserved space management
effectively shrinks unused reserved space with limited reclaim overhead. 

%We assign a primary OSD which handles encoding and chunk distribution for
%each segment. Our scalable design offloads client-side encoding computations
%to the distributed file system. We minimize the network traffic for parity
%updates by calculating a {\em parity delta} which can be used to reconstruct
%the parity chunk during degraded read and recovery.  CodFS implements
%different delta-based parity update schemes. 


%Second, we design a set of storage schemes for the data and parity chunks. We
%show that although a traditional log-structured file system, which we name it
%the \textit{full-logging} performs well in updates, it suffers from poor
%sequential read performance and slow recovery compared with the other
%schemes. The previously proposed \textit{parity-logging} (\PL) scheme is a
%hybrid method that sits between the \FL scheme the \textit{full-overwrite}
%(\FO) scheme. We show that the \PL scheme is comparable with the \FL scheme
%in updates but performs significantly better in sequential read. Based on the
%\PL scheme, we proposed the \textit{parity-logging with reserved space}
%(\PLR) scheme which keeps the parity deltas close to the parity chunks. This
%further improves the recovery throughput by reducing the number of disk
%seeks. We also describe heuristics for limiting the size of reserved space by
%shrinking and merging.

%Third, we incorporate end-to-end integrity into our design by computing and
%verifying checksums in the primary OSD. The checksum ensures that corrupt data
%never reaches the clients and redundancy provided by erasure coding can be used
%to recover corrupt data upon detection. We showed that CRC using Intel SSE4.2
%instruction set introduces negligible overhead. We also show that we can apply
%the storage schemes used in normal blocks to checksum blocks to improve overall
%I/O performance.

\section{Organization}

%Organization
The rest of the thesis proceeds as follows. 

\begin{itemize}
    \item 
        \textbf{Chapter~\ref{chap:background}}\\
        We introduce the background of erasure coding and describe the existing
        parity update approaches. We also review related work regarding various 
        aspects of the project.
    \item
        \textbf{Chapter~\ref{chap:motivation}}\\
        We describe the motivation of our design by analyzing the update
        behavior of real-world traces. 
    \item 
        \textbf{Chapter~\ref{chap:codfs}}\\
        We propose a new parity update approach and 
        address the design and implementation of CodFS. 
    \item 
        \textbf{Chapter~\ref{chap:evaluation}}\\ 
        We present testbed experimental results. 
    \item 
        \textbf{Chapter~\ref{chap:tween}}\\
        We introduce the goals and design of TWEEN, which improves write
        efficiency and endurance of an SSD RAID using non-volatile cache.
    \item 
        \textbf{Chapter~\ref{chap:conclusions}}\\
        We summarize the thesis and describe future work.
\end{itemize}

